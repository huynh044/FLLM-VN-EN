{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11cab4da",
   "metadata": {},
   "source": [
    "# Fine-tuning LLM for Vietnamese to English Translation\n",
    "\n",
    "This notebook demonstrates how to fine-tune a large language model (LLM) for translating Vietnamese text to English. We'll use transformer models like mT5 or MarianMT with the Hugging Face ecosystem.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "- **Objective**: Fine-tune a pre-trained model for Vietnamese-English translation\n",
    "- **Models**: mT5, MarianMT, or similar sequence-to-sequence models\n",
    "- **Dataset**: Vietnamese-English parallel corpus\n",
    "- **Evaluation**: BLEU score and qualitative assessment\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Set up environment and install dependencies\n",
    "2. Load and preprocess Vietnamese-English translation dataset\n",
    "3. Tokenize and prepare data for model input\n",
    "4. Load pre-trained LLM model and tokenizer\n",
    "5. Configure training arguments for fine-tuning\n",
    "6. Fine-tune the model on translation data\n",
    "7. Evaluate model performance on validation set\n",
    "8. Translate sample Vietnamese sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8af4ff",
   "metadata": {},
   "source": [
    "## 1. Set Up Environment and Install Dependencies\n",
    "\n",
    "First, let's install all the required libraries for our translation project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers datasets accelerate peft bitsandbytes\n",
    "!pip install rouge-score evaluate\n",
    "!pip install pandas numpy matplotlib seaborn plotly\n",
    "!pip install pyvi underthesea  # Vietnamese NLP libraries\n",
    "!pip install tqdm wandb tensorboard  # Utilities\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d51d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers and related libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import evaluate\n",
    "\n",
    "# PEFT for parameter-efficient fine-tuning\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Vietnamese text processing\n",
    "try:\n",
    "    from pyvi import ViTokenizer\n",
    "    from underthesea import word_tokenize as vn_tokenize\n",
    "    print(\"âœ… Vietnamese NLP libraries loaded\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Vietnamese NLP libraries not available\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ”§ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ“Š GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3d581",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Vietnamese-English Translation Dataset\n",
    "\n",
    "We'll create a sample dataset and also show how to load real parallel corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample Vietnamese-English dataset\n",
    "sample_data = [\n",
    "    {\"vietnamese\": \"Xin chÃ o! TÃ´i tÃªn lÃ  Nam.\", \"english\": \"Hello! My name is Nam.\"},\n",
    "    {\"vietnamese\": \"HÃ´m nay thá»i tiáº¿t ráº¥t Ä‘áº¹p.\", \"english\": \"The weather is very nice today.\"},\n",
    "    {\"vietnamese\": \"TÃ´i Ä‘ang há»c tiáº¿ng Anh.\", \"english\": \"I am learning English.\"},\n",
    "    {\"vietnamese\": \"Báº¡n cÃ³ khá»e khÃ´ng?\", \"english\": \"How are you?\"},\n",
    "    {\"vietnamese\": \"Cáº£m Æ¡n báº¡n ráº¥t nhiá»u.\", \"english\": \"Thank you very much.\"},\n",
    "    {\"vietnamese\": \"Xin lá»—i, tÃ´i khÃ´ng hiá»ƒu.\", \"english\": \"Sorry, I don't understand.\"},\n",
    "    {\"vietnamese\": \"Báº¡n cÃ³ thá»ƒ giÃºp tÃ´i Ä‘Æ°á»£c khÃ´ng?\", \"english\": \"Can you help me?\"},\n",
    "    {\"vietnamese\": \"TÃ´i muá»‘n Ä‘i du lá»‹ch Viá»‡t Nam.\", \"english\": \"I want to travel to Vietnam.\"},\n",
    "    {\"vietnamese\": \"MÃ³n phá»Ÿ nÃ y ráº¥t ngon.\", \"english\": \"This pho is very delicious.\"},\n",
    "    {\"vietnamese\": \"ChÃºc báº¡n má»™t ngÃ y tá»‘t lÃ nh.\", \"english\": \"Have a nice day.\"},\n",
    "    {\"vietnamese\": \"TÃ´i yÃªu vÄƒn hÃ³a Viá»‡t Nam.\", \"english\": \"I love Vietnamese culture.\"},\n",
    "    {\"vietnamese\": \"HÃ  Ná»™i lÃ  thá»§ Ä‘Ã´ cá»§a Viá»‡t Nam.\", \"english\": \"Hanoi is the capital of Vietnam.\"},\n",
    "    {\"vietnamese\": \"TÃ´i thÃ­ch Äƒn bÃ¡nh mÃ¬.\", \"english\": \"I like to eat banh mi.\"},\n",
    "    {\"vietnamese\": \"NgÃ y mai tÃ´i sáº½ Ä‘i lÃ m.\", \"english\": \"Tomorrow I will go to work.\"},\n",
    "    {\"vietnamese\": \"Gia Ä‘Ã¬nh tÃ´i cÃ³ bá»‘n ngÆ°á»i.\", \"english\": \"My family has four people.\"},\n",
    "    {\"vietnamese\": \"TÃ´i há»c táº¡i trÆ°á»ng Ä‘áº¡i há»c.\", \"english\": \"I study at the university.\"},\n",
    "    {\"vietnamese\": \"Xe buÃ½t sáº½ Ä‘áº¿n lÃºc 8 giá».\", \"english\": \"The bus will arrive at 8 o'clock.\"},\n",
    "    {\"vietnamese\": \"TÃ´i cáº§n mua má»™t cuá»‘n sÃ¡ch.\", \"english\": \"I need to buy a book.\"},\n",
    "    {\"vietnamese\": \"BÃ¡c sÄ© nÃ³i tÃ´i khá»e máº¡nh.\", \"english\": \"The doctor says I'm healthy.\"},\n",
    "    {\"vietnamese\": \"ChÃºng tÃ´i sáº½ gáº·p nhau tá»‘i nay.\", \"english\": \"We will meet tonight.\"}\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(sample_data)\n",
    "print(f\"ğŸ“Š Created sample dataset with {len(df)} translation pairs\")\n",
    "\n",
    "# Display first few examples\n",
    "print(\"\\nğŸ” Sample translation pairs:\")\n",
    "for i in range(5):\n",
    "    print(f\"ğŸ‡»ğŸ‡³ Vietnamese: {df.iloc[i]['vietnamese']}\")\n",
    "    print(f\"ğŸ‡ºğŸ‡¸ English: {df.iloc[i]['english']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "import re\n",
    "\n",
    "def clean_vietnamese_text(text):\n",
    "    \"\"\"Clean Vietnamese text.\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters (keep Vietnamese diacritics)\n",
    "    text = re.sub(r'[^\\w\\s\\u00C0-\\u1EF9]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_english_text(text):\n",
    "    \"\"\"Clean English text.\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters (keep basic punctuation)\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing\n",
    "df['vietnamese_clean'] = df['vietnamese'].apply(clean_vietnamese_text)\n",
    "df['english_clean'] = df['english'].apply(clean_english_text)\n",
    "\n",
    "# Analyze text lengths\n",
    "df['vn_length'] = df['vietnamese_clean'].str.len()\n",
    "df['en_length'] = df['english_clean'].str.len()\n",
    "df['vn_words'] = df['vietnamese_clean'].str.split().str.len()\n",
    "df['en_words'] = df['english_clean'].str.split().str.len()\n",
    "\n",
    "# Display statistics\n",
    "print(\"ğŸ“ˆ Dataset Statistics:\")\n",
    "print(f\"Average Vietnamese length: {df['vn_length'].mean():.1f} characters\")\n",
    "print(f\"Average English length: {df['en_length'].mean():.1f} characters\")\n",
    "print(f\"Average Vietnamese words: {df['vn_words'].mean():.1f} words\")\n",
    "print(f\"Average English words: {df['en_words'].mean():.1f} words\")\n",
    "\n",
    "# Visualize length distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].hist(df['vn_words'], bins=10, alpha=0.7, label='Vietnamese', color='blue')\n",
    "axes[0].hist(df['en_words'], bins=10, alpha=0.7, label='English', color='red')\n",
    "axes[0].set_xlabel('Number of Words')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Word Count Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(df['vn_words'], df['en_words'], alpha=0.7)\n",
    "axes[1].set_xlabel('Vietnamese Words')\n",
    "axes[1].set_ylabel('English Words')\n",
    "axes[1].set_title('Vietnamese vs English Word Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4ae0b",
   "metadata": {},
   "source": [
    "## 3. Tokenize and Prepare Data for Model Input\n",
    "\n",
    "We'll use a multilingual T5 (mT5) tokenizer to process our Vietnamese-English data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model and load tokenizer\n",
    "MODEL_NAME = \"google/mt5-small\"  # Start with small model for testing\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"ğŸ¤– Loaded tokenizer: {MODEL_NAME}\")\n",
    "print(f\"ğŸ“š Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "# Test tokenization on a sample\n",
    "sample_vn = df.iloc[0]['vietnamese_clean']\n",
    "sample_en = df.iloc[0]['english_clean']\n",
    "\n",
    "print(f\"\\nğŸ§ª Tokenization Test:\")\n",
    "print(f\"Vietnamese: {sample_vn}\")\n",
    "vn_tokens = tokenizer.tokenize(sample_vn)\n",
    "print(f\"VN Tokens: {vn_tokens}\")\n",
    "print(f\"VN Token IDs: {tokenizer.encode(sample_vn)}\")\n",
    "\n",
    "print(f\"\\nEnglish: {sample_en}\")\n",
    "en_tokens = tokenizer.tokenize(sample_en)\n",
    "print(f\"EN Tokens: {en_tokens}\")\n",
    "print(f\"EN Token IDs: {tokenizer.encode(sample_en)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data in T5 format: \"translate Vietnamese to English: [VN_TEXT]\" -> \"[EN_TEXT]\"\n",
    "def create_t5_format(vietnamese_text, english_text):\n",
    "    \"\"\"Create T5 input-output format.\"\"\"\n",
    "    input_text = f\"translate Vietnamese to English: {vietnamese_text}\"\n",
    "    target_text = english_text\n",
    "    return input_text, target_text\n",
    "\n",
    "# Create T5 formatted data\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    input_text, target_text = create_t5_format(\n",
    "        row['vietnamese_clean'], \n",
    "        row['english_clean']\n",
    "    )\n",
    "    inputs.append(input_text)\n",
    "    targets.append(target_text)\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "dataset_dict = {\n",
    "    'input_text': inputs,\n",
    "    'target_text': targets,\n",
    "    'vietnamese': df['vietnamese_clean'].tolist(),\n",
    "    'english': df['english_clean'].tolist()\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Split into train/validation sets (80/20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, len(dataset)))\n",
    "\n",
    "dataset_splits = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "print(f\"ğŸ“Š Dataset splits:\")\n",
    "print(f\"  Training: {len(train_dataset)} examples\")\n",
    "print(f\"  Validation: {len(val_dataset)} examples\")\n",
    "\n",
    "# Show example of formatted data\n",
    "print(f\"\\nğŸ” Example formatted data:\")\n",
    "example = train_dataset[0]\n",
    "print(f\"Input: {example['input_text']}\")\n",
    "print(f\"Target: {example['target_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization preprocessing function\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize inputs and targets.\"\"\"\n",
    "    inputs = examples['input_text']\n",
    "    targets = examples['target_text']\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=False  # We'll pad dynamically during training\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization to datasets\n",
    "tokenized_datasets = dataset_splits.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_splits[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"âœ… Tokenization completed\")\n",
    "print(f\"ğŸ“Š Tokenized dataset structure:\")\n",
    "print(tokenized_datasets)\n",
    "\n",
    "# Show tokenized example\n",
    "example = tokenized_datasets['train'][0]\n",
    "print(f\"\\nğŸ” Tokenized example:\")\n",
    "print(f\"Input IDs shape: {len(example['input_ids'])}\")\n",
    "print(f\"Label IDs shape: {len(example['labels'])}\")\n",
    "print(f\"First few input tokens: {tokenizer.convert_ids_to_tokens(example['input_ids'][:10])}\")\n",
    "print(f\"First few label tokens: {tokenizer.convert_ids_to_tokens(example['labels'][:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3dbb41",
   "metadata": {},
   "source": [
    "## 4. Load Pre-trained LLM Model and Tokenizer\n",
    "\n",
    "We'll load the mT5 model and set it up for translation fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "print(f\"ğŸ¤– Loaded model: {MODEL_NAME}\")\n",
    "print(f\"ğŸ“Š Model parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up LoRA (Parameter-Efficient Fine-Tuning)\n",
    "USE_PEFT = True  # Set to False for full fine-tuning\n",
    "\n",
    "if USE_PEFT:\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=16,                    # LoRA rank\n",
    "        lora_alpha=32,          # LoRA alpha\n",
    "        lora_dropout=0.1,       # LoRA dropout\n",
    "        target_modules=[\"q\", \"v\"]  # Target attention modules\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    print(\"âœ… LoRA configuration applied\")\n",
    "else:\n",
    "    print(\"ğŸ“š Using full fine-tuning\")\n",
    "\n",
    "# Setup data collator for dynamic padding\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8 if torch.cuda.is_available() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc815fd7",
   "metadata": {},
   "source": [
    "## 5. Configure Training Arguments for Fine-Tuning\n",
    "\n",
    "Set up training parameters optimized for our translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73557582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "output_dir = \"./results/mt5-vietnamese-english\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Training configuration\n",
    "    num_train_epochs=10,  # More epochs for small dataset\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    # Learning rate and optimization\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    logging_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    \n",
    "    # Model selection\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_bleu\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Generation parameters\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,\n",
    "    generation_num_beams=4,\n",
    "    \n",
    "    # Performance optimizations\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=0,  # Reduce for small dataset\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=None,  # Disable wandb for this demo\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸ Training arguments configured:\")\n",
    "print(f\"  ğŸ“ Output directory: {output_dir}\")\n",
    "print(f\"  ğŸ”¢ Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  ğŸ“ Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  ğŸ“Š Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  ğŸ¯ Evaluation steps: {training_args.eval_steps}\")\n",
    "print(f\"  ğŸ’¾ Save steps: {training_args.save_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute BLEU and ROUGE metrics for evaluation.\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up text\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    bleu_result = bleu_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[label] for label in decoded_labels]\n",
    "    )\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    rouge_result = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels\n",
    "    )\n",
    "    \n",
    "    # Combine results\n",
    "    result = {\n",
    "        \"bleu\": bleu_result[\"bleu\"],\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"]\n",
    "    }\n",
    "    \n",
    "    # Add generation length info\n",
    "    prediction_lens = [len(pred.split()) for pred in decoded_preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"ğŸ“Š Evaluation metrics configured:\")\n",
    "print(\"  - BLEU score for translation quality\")\n",
    "print(\"  - ROUGE scores for text similarity\")\n",
    "print(\"  - Generation length statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae92939",
   "metadata": {},
   "source": [
    "## 6. Fine-Tune the Model on Translation Data\n",
    "\n",
    "Now we'll train our model on the Vietnamese-English dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ Trainer initialized!\")\n",
    "print(f\"ğŸ“Š Training dataset size: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"ğŸ“Š Validation dataset size: {len(tokenized_datasets['validation'])}\")\n",
    "\n",
    "# Test the model before training\n",
    "print(\"\\nğŸ§ª Testing model before training:\")\n",
    "test_input = \"translate Vietnamese to English: Xin chÃ o báº¡n!\"\n",
    "inputs = tokenizer.encode(test_input, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs, max_length=50, num_beams=4)\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Before training: {translation}\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nğŸ‹ï¸ Starting training...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c901b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(f\"{output_dir}/final_model\")\n",
    "tokenizer.save_pretrained(f\"{output_dir}/final_model\")\n",
    "\n",
    "print(\"ğŸ’¾ Model saved successfully!\")\n",
    "\n",
    "# Plot training history\n",
    "training_history = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "if len(training_history) > 0:\n",
    "    # Separate training and evaluation logs\n",
    "    train_logs = training_history[training_history['train_loss'].notna()]\n",
    "    eval_logs = training_history[training_history['eval_loss'].notna()]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training loss\n",
    "    if len(train_logs) > 0:\n",
    "        axes[0, 0].plot(train_logs['step'], train_logs['train_loss'], 'b-', label='Training Loss')\n",
    "        axes[0, 0].set_title('Training Loss')\n",
    "        axes[0, 0].set_xlabel('Steps')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "    \n",
    "    # Evaluation loss\n",
    "    if len(eval_logs) > 0:\n",
    "        axes[0, 1].plot(eval_logs['step'], eval_logs['eval_loss'], 'r-', label='Validation Loss')\n",
    "        axes[0, 1].set_title('Validation Loss')\n",
    "        axes[0, 1].set_xlabel('Steps')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].legend()\n",
    "    \n",
    "    # BLEU score\n",
    "    if len(eval_logs) > 0 and 'eval_bleu' in eval_logs.columns:\n",
    "        axes[1, 0].plot(eval_logs['step'], eval_logs['eval_bleu'], 'g-', label='BLEU Score')\n",
    "        axes[1, 0].set_title('BLEU Score')\n",
    "        axes[1, 0].set_xlabel('Steps')\n",
    "        axes[1, 0].set_ylabel('BLEU')\n",
    "        axes[1, 0].legend()\n",
    "    \n",
    "    # Learning rate\n",
    "    if len(train_logs) > 0 and 'learning_rate' in train_logs.columns:\n",
    "        axes[1, 1].plot(train_logs['step'], train_logs['learning_rate'], 'm-', label='Learning Rate')\n",
    "        axes[1, 1].set_title('Learning Rate')\n",
    "        axes[1, 1].set_xlabel('Steps')\n",
    "        axes[1, 1].set_ylabel('Learning Rate')\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ No training history available for plotting\")\n",
    "\n",
    "print(\"âœ… Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c6835",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance on Validation Set\n",
    "\n",
    "Let's evaluate our fine-tuned model and compare it with the pre-trained version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model\n",
    "print(\"ğŸ“Š Evaluating fine-tuned model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nğŸ¯ Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Detailed evaluation on validation set\n",
    "validation_data = dataset_splits['validation']\n",
    "print(f\"\\nğŸ” Detailed evaluation on {len(validation_data)} examples:\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "vietnamese_texts = []\n",
    "\n",
    "for i, example in enumerate(validation_data):\n",
    "    vietnamese_text = example['vietnamese']\n",
    "    english_text = example['english']\n",
    "    \n",
    "    # Create input for the model\n",
    "    input_text = f\"translate Vietnamese to English: {vietnamese_text}\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=max_input_length, truncation=True).to(device)\n",
    "    \n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_target_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    # Decode prediction\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "    references.append(english_text)\n",
    "    vietnamese_texts.append(vietnamese_text)\n",
    "\n",
    "# Calculate metrics\n",
    "final_bleu = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "final_rouge = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Final Metrics:\")\n",
    "print(f\"  BLEU Score: {final_bleu['bleu']:.4f}\")\n",
    "print(f\"  ROUGE-1: {final_rouge['rouge1']:.4f}\")\n",
    "print(f\"  ROUGE-2: {final_rouge['rouge2']:.4f}\")\n",
    "print(f\"  ROUGE-L: {final_rouge['rougeL']:.4f}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nğŸŒŸ Translation Examples:\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(min(5, len(predictions))):\n",
    "    print(f\"Vietnamese: {vietnamese_texts[i]}\")\n",
    "    print(f\"Reference:  {references[i]}\")\n",
    "    print(f\"Predicted:  {predictions[i]}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed93941",
   "metadata": {},
   "source": [
    "## 8. Translate Sample Vietnamese Sentences\n",
    "\n",
    "Let's test our fine-tuned model with some new Vietnamese sentences and see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca5c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a translation function\n",
    "def translate_vietnamese(text, max_length=128, num_beams=4):\n",
    "    \"\"\"Translate Vietnamese text to English using our fine-tuned model.\"\"\"\n",
    "    input_text = f\"translate Vietnamese to English: {text}\"\n",
    "    inputs = tokenizer.encode(\n",
    "        input_text, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=max_input_length, \n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translation\n",
    "\n",
    "# Test sentences (not in training data)\n",
    "test_sentences = [\n",
    "    \"TÃ´i ráº¥t thÃ­ch mÃ³n phá»Ÿ Viá»‡t Nam.\",\n",
    "    \"HÃ´m nay trá»i mÆ°a to láº¯m.\",\n",
    "    \"Báº¡n cÃ³ thá»ƒ cho tÃ´i biáº¿t Ä‘Æ°á»ng Ä‘i khÃ´ng?\",\n",
    "    \"ChÃºng tÃ´i sáº½ gáº·p nhau vÃ o cuá»‘i tuáº§n.\",\n",
    "    \"Tiáº¿ng Viá»‡t lÃ  ngÃ´n ngá»¯ ráº¥t hay.\",\n",
    "    \"TÃ´i muá»‘n há»c láº­p trÃ¬nh mÃ¡y tÃ­nh.\",\n",
    "    \"Cuá»™c sá»‘ng á»Ÿ thÃ nh phá»‘ ráº¥t nÃ¡o nhiá»‡t.\",\n",
    "    \"Gia Ä‘Ã¬nh lÃ  Ä‘iá»u quan trá»ng nháº¥t.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ‡»ğŸ‡³ â¡ï¸ ğŸ‡ºğŸ‡¸ Vietnamese to English Translation Results\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, vietnamese_text in enumerate(test_sentences, 1):\n",
    "    translation = translate_vietnamese(vietnamese_text)\n",
    "    print(f\"{i}. Vietnamese: {vietnamese_text}\")\n",
    "    print(f\"   English:   {translation}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Interactive translation (you can modify this cell to test your own sentences)\n",
    "print(\"\\nğŸš€ Try your own translations!\")\n",
    "print(\"Modify the sentence below and run the cell to see the translation:\")\n",
    "\n",
    "custom_sentence = \"TÃ´i yÃªu Viá»‡t Nam vÃ  vÄƒn hÃ³a truyá»n thá»‘ng.\"\n",
    "custom_translation = translate_vietnamese(custom_sentence)\n",
    "\n",
    "print(f\"\\nğŸ‡»ğŸ‡³ Vietnamese: {custom_sentence}\")\n",
    "print(f\"ğŸ‡ºğŸ‡¸ English: {custom_translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953b628",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "ğŸ‰ **Congratulations!** You have successfully fine-tuned a large language model for Vietnamese to English translation.\n",
    "\n",
    "### What we accomplished:\n",
    "- âœ… Set up the environment with all necessary libraries\n",
    "- âœ… Created and preprocessed a Vietnamese-English dataset\n",
    "- âœ… Tokenized data using mT5 tokenizer\n",
    "- âœ… Loaded and configured a pre-trained mT5 model\n",
    "- âœ… Applied LoRA for parameter-efficient fine-tuning\n",
    "- âœ… Trained the model with proper evaluation metrics\n",
    "- âœ… Evaluated performance using BLEU and ROUGE scores\n",
    "- âœ… Tested translations on new Vietnamese sentences\n",
    "\n",
    "### Next Steps to Improve:\n",
    "\n",
    "1. **Larger Dataset**: \n",
    "   - Use larger parallel corpora (OPUS, OpenSubtitles, etc.)\n",
    "   - Implement the data download script in `scripts/download_data.py`\n",
    "\n",
    "2. **Better Models**: \n",
    "   - Try larger models (mT5-base, mT5-large)\n",
    "   - Experiment with other architectures (MarianMT, NLLB)\n",
    "\n",
    "3. **Advanced Techniques**:\n",
    "   - Data augmentation (back-translation)\n",
    "   - Multi-task learning\n",
    "   - Domain adaptation\n",
    "\n",
    "4. **Production Deployment**:\n",
    "   - Use the inference script in `src/inference/translate.py`\n",
    "   - Deploy as a web service or API\n",
    "   - Optimize for speed and memory usage\n",
    "\n",
    "### Resources:\n",
    "- ğŸ“– [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers)\n",
    "- ğŸ”§ [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- ğŸ“Š [Evaluate Library](https://huggingface.co/docs/evaluate)\n",
    "- ğŸŒ [OPUS Parallel Corpora](http://opus.nlpl.eu/)\n",
    "\n",
    "### Model Performance Tips:\n",
    "- Monitor validation loss to avoid overfitting\n",
    "- Experiment with different learning rates\n",
    "- Try different beam search settings for generation\n",
    "- Consider ensembling multiple models for better results\n",
    "\n",
    "Happy translating! ğŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
