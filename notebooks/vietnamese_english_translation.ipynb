{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11cab4da",
   "metadata": {},
   "source": [
    "# Vietnamese-English Translation vá»›i PyTorch 2.5.1\n",
    "\n",
    "**HÆ°á»›ng dáº«n Ä‘Æ¡n giáº£n Ä‘á»ƒ fine-tune model dá»‹ch Viá»‡t-Anh**\n",
    "\n",
    "## ðŸŽ¯ Má»¥c tiÃªu\n",
    "- Fine-tune model mT5 cho dá»‹ch thuáº­t Viá»‡t-Anh\n",
    "- Sá»­ dá»¥ng LoRA Ä‘á»ƒ training hiá»‡u quáº£\n",
    "- TÆ°Æ¡ng thÃ­ch hoÃ n toÃ n vá»›i PyTorch 2.5.1+cu121\n",
    "\n",
    "## ðŸ“‹ CÃ¡c bÆ°á»›c thá»±c hiá»‡n\n",
    "1. âœ… CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t\n",
    "2. âœ… Táº¡o dataset Vietnamese-English\n",
    "3. âœ… Load vÃ  chuáº©n bá»‹ model mT5\n",
    "4. âœ… Setup LoRA cho efficient training\n",
    "5. âœ… Training vá»›i parameters tá»‘i Æ°u\n",
    "6. âœ… Test vÃ  Ä‘Ã¡nh giÃ¡ káº¿t quáº£\n",
    "\n",
    "## ðŸ”§ YÃªu cáº§u há»‡ thá»‘ng\n",
    "- Python 3.8+\n",
    "- PyTorch 2.5.1+cu121\n",
    "- GPU vá»›i CUDA 12.1 (hoáº·c CPU)\n",
    "- RAM: 8GB+ (16GB khuyáº¿n nghá»‹)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8af4ff",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 1: CÃ i Ä‘áº·t thÆ° viá»‡n\n",
    "\n",
    "**Quan trá»ng:** Sá»­ dá»¥ng PyTorch 2.5.1+cu121 Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch tá»‘t nháº¥t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÃ i Ä‘áº·t PyTorch 2.5.1 vá»›i CUDA 12.1\n",
    "!pip install torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# CÃ i Ä‘áº·t thÆ° viá»‡n machine learning\n",
    "!pip install transformers>=4.44.0 datasets>=2.20.0 accelerate>=0.33.0 peft>=0.12.0 safetensors>=0.4.0\n",
    "\n",
    "# CÃ i Ä‘áº·t thÆ° viá»‡n Ä‘Ã¡nh giÃ¡\n",
    "!pip install evaluate rouge-score\n",
    "\n",
    "# CÃ i Ä‘áº·t thÆ° viá»‡n phá»¥ trá»£\n",
    "!pip install pandas numpy matplotlib tqdm\n",
    "\n",
    "print(\"âœ… ÄÃ£ cÃ i Ä‘áº·t táº¥t cáº£ thÆ° viá»‡n cáº§n thiáº¿t!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d51d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import thÆ° viá»‡n cáº§n thiáº¿t\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "# PEFT cho LoRA\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Kiá»ƒm tra GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ”§ Sá»­ dá»¥ng device: {device}\")\n",
    "print(f\"ðŸ Python version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸŽ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"ðŸ”¥ CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Cháº¡y trÃªn CPU - sáº½ cháº­m hÆ¡n\")\n",
    "\n",
    "# Äáº·t random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(\"âœ… Setup hoÃ n táº¥t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3d581",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 2: Táº¡o Dataset Vietnamese-English\n",
    "\n",
    "Táº¡o dataset Ä‘Æ¡n giáº£n Ä‘á»ƒ demo. Trong thá»±c táº¿, báº¡n sáº½ sá»­ dá»¥ng dataset lá»›n hÆ¡n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Táº¡o dataset Vietnamese-English Ä‘Æ¡n giáº£n, dataaset sáº½ trÃ´ng nhÆ° tháº¿ nÃ y:\n",
    "data = [\n",
    "    {\"vn\": \"Xin chÃ o\", \"en\": \"Hello\"},\n",
    "    {\"vn\": \"Táº¡m biá»‡t\", \"en\": \"Goodbye\"},\n",
    "    {\"vn\": \"Cáº£m Æ¡n báº¡n\", \"en\": \"Thank you\"},\n",
    "    {\"vn\": \"Xin lá»—i\", \"en\": \"Sorry\"},\n",
    "    {\"vn\": \"Báº¡n khá»e khÃ´ng?\", \"en\": \"How are you?\"},\n",
    "    {\"vn\": \"TÃ´i tÃªn lÃ  Nam\", \"en\": \"My name is Nam\"},\n",
    "    {\"vn\": \"HÃ´m nay thá»i tiáº¿t Ä‘áº¹p\", \"en\": \"The weather is nice today\"},\n",
    "    {\"vn\": \"TÃ´i thÃ­ch Äƒn phá»Ÿ\", \"en\": \"I like to eat pho\"},\n",
    "    {\"vn\": \"ChÃºc báº¡n ngá»§ ngon\", \"en\": \"Good night\"},\n",
    "    {\"vn\": \"Háº¹n gáº·p láº¡i\", \"en\": \"See you later\"},\n",
    "    {\"vn\": \"TÃ´i Ä‘ang há»c tiáº¿ng Anh\", \"en\": \"I am learning English\"},\n",
    "    {\"vn\": \"Báº¡n cÃ³ thá»ƒ giÃºp tÃ´i khÃ´ng?\", \"en\": \"Can you help me?\"},\n",
    "    {\"vn\": \"TÃ´i yÃªu Viá»‡t Nam\", \"en\": \"I love Vietnam\"},\n",
    "    {\"vn\": \"HÃ  Ná»™i lÃ  thá»§ Ä‘Ã´\", \"en\": \"Hanoi is the capital\"},\n",
    "    {\"vn\": \"MÃ³n nÃ y ráº¥t ngon\", \"en\": \"This dish is delicious\"},\n",
    "    {\"vn\": \"TÃ´i muá»‘n Ä‘i du lá»‹ch\", \"en\": \"I want to travel\"},\n",
    "    {\"vn\": \"Gia Ä‘Ã¬nh tÃ´i cÃ³ 4 ngÆ°á»i\", \"en\": \"My family has 4 people\"},\n",
    "    {\"vn\": \"TÃ´i lÃ m viá»‡c á»Ÿ vÄƒn phÃ²ng\", \"en\": \"I work at the office\"},\n",
    "    {\"vn\": \"ChÃºng ta Ä‘i Äƒn nhÃ©\", \"en\": \"Let's go eat\"},\n",
    "    {\"vn\": \"TÃ´i thÃ­ch xem phim\", \"en\": \"I like watching movies\"}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"ðŸ“Š Táº¡o dataset vá»›i {len(df)} cáº·p cÃ¢u\")\n",
    "print(\"\\nðŸ” Má»™t sá»‘ vÃ­ dá»¥:\")\n",
    "for i in range(3):\n",
    "    print(f\"ðŸ‡»ðŸ‡³ {df.iloc[i]['vn']} â†’ ðŸ‡ºðŸ‡¸ {df.iloc[i]['en']}\")\n",
    "    \n",
    "print(f\"\\nðŸ“ˆ Thá»‘ng kÃª:\")\n",
    "print(f\"Trung bÃ¬nh tá»« tiáº¿ng Viá»‡t: {df['vn'].str.split().str.len().mean():.1f}\")\n",
    "print(f\"Trung bÃ¬nh tá»« tiáº¿ng Anh: {df['en'].str.split().str.len().mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuáº©n bá»‹ data cho T5 format\n",
    "def create_t5_format(vn_text, en_text):\n",
    "    \"\"\"Táº¡o format cho T5: 'translate Vietnamese to English: [VN]' -> '[EN]'\"\"\"\n",
    "    input_text = f\"translate Vietnamese to English: {vn_text}\"\n",
    "    target_text = en_text\n",
    "    return input_text, target_text\n",
    "\n",
    "# Táº¡o dataset vá»›i T5 format\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    inp, tgt = create_t5_format(row['vn'], row['en'])\n",
    "    inputs.append(inp)\n",
    "    targets.append(tgt)\n",
    "\n",
    "print(\"ðŸ”„ VÃ­ dá»¥ T5 format:\")\n",
    "print(f\"Input: {inputs[0]}\")\n",
    "print(f\"Target: {targets[0]}\")\n",
    "\n",
    "# Táº¡o Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    'input_text': inputs,\n",
    "    'target_text': targets\n",
    "})\n",
    "\n",
    "# Chia train/validation (80/20)\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, len(dataset)))\n",
    "\n",
    "print(f\"\\nðŸ“Š Chia dataset:\")\n",
    "print(f\"Training: {len(train_dataset)} samples\")\n",
    "print(f\"Validation: {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4ae0b",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 3: Load Model vÃ  Tokenizer\n",
    "\n",
    "Sá»­ dá»¥ng mT5-small - model nhá» nhÆ°ng hiá»‡u quáº£ cho demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model vÃ  tokenizer\n",
    "MODEL_NAME = \"google/mt5-small\"\n",
    "print(f\"ðŸ“¥ Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"ðŸ“š Tokenizer vocabulary: {len(tokenizer):,} tokens\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"translate Vietnamese to English: Xin chÃ o\"\n",
    "tokens = tokenizer.tokenize(test_text)\n",
    "print(f\"\\nðŸ§ª Test tokenization:\")\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Tokens: {tokens[:10]}...\")  # Chá»‰ hiá»ƒn thá»‹ 10 tokens Ä‘áº§u\n",
    "print(f\"Token IDs: {tokenizer.encode(test_text)[:10]}...\")\n",
    "\n",
    "print(\"âœ… Tokenizer sáºµn sÃ ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data in T5 format: \"translate Vietnamese to English: [VN_TEXT]\" -> \"[EN_TEXT]\"\n",
    "def create_t5_format(vietnamese_text, english_text):\n",
    "    \"\"\"Create T5 input-output format.\"\"\"\n",
    "    input_text = f\"translate Vietnamese to English: {vietnamese_text}\"\n",
    "    target_text = english_text\n",
    "    return input_text, target_text\n",
    "\n",
    "# Create T5 formatted data\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    input_text, target_text = create_t5_format(\n",
    "        row['vietnamese_clean'], \n",
    "        row['english_clean']\n",
    "    )\n",
    "    inputs.append(input_text)\n",
    "    targets.append(target_text)\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "dataset_dict = {\n",
    "    'input_text': inputs,\n",
    "    'target_text': targets,\n",
    "    'vietnamese': df['vietnamese_clean'].tolist(),\n",
    "    'english': df['english_clean'].tolist()\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Split into train/validation sets (80/20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, len(dataset)))\n",
    "\n",
    "dataset_splits = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "print(f\"ðŸ“Š Dataset splits:\")\n",
    "print(f\"  Training: {len(train_dataset)} examples\")\n",
    "print(f\"  Validation: {len(val_dataset)} examples\")\n",
    "\n",
    "# Show example of formatted data\n",
    "print(f\"\\nðŸ” Example formatted data:\")\n",
    "example = train_dataset[0]\n",
    "print(f\"Input: {example['input_text']}\")\n",
    "print(f\"Target: {example['target_text']}\")\n",
    "\n",
    "# Tokenize dataset\n",
    "max_length = 128\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize input vÃ  target texts\"\"\"\n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(\n",
    "        examples['input_text'],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=False  # Padding Ä‘á»™ng trong training\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    targets = tokenizer(\n",
    "        examples['target_text'],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Tokenize cáº£ train vÃ  validation\n",
    "print(\"ðŸ”„ Tokenizing dataset...\")\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# XÃ³a columns khÃ´ng cáº§n thiáº¿t\n",
    "train_dataset = train_dataset.remove_columns(['input_text', 'target_text'])\n",
    "val_dataset = val_dataset.remove_columns(['input_text', 'target_text'])\n",
    "\n",
    "print(f\"âœ… Tokenization hoÃ n táº¥t!\")\n",
    "print(f\"ðŸ“Š Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"ðŸ“Š Val dataset: {len(val_dataset)} samples\")\n",
    "\n",
    "# Kiá»ƒm tra 1 sample\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nðŸ” Sample tokenized:\")\n",
    "print(f\"Input length: {len(sample['input_ids'])}\")\n",
    "print(f\"Labels length: {len(sample['labels'])}\")\n",
    "print(f\"Input tokens: {tokenizer.convert_ids_to_tokens(sample['input_ids'][:5])}\")\n",
    "print(f\"Label tokens: {tokenizer.convert_ids_to_tokens(sample['labels'][:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3dbb41",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 4: Load Model vÃ  Setup LoRA\n",
    "\n",
    "TÆ°Æ¡ng thÃ­ch vá»›i PyTorch 2.5.1+cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model vá»›i PyTorch 2.5.1 compatibility\n",
    "print(f\"ðŸ“¥ Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load model vá»›i float32 cho stability\n",
    "try:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32,  # Sá»­ dá»¥ng float32 cho PyTorch 2.5.1\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    print(\"âœ… Model loaded vá»›i safetensors\")\n",
    "except:\n",
    "    print(\"âš ï¸ Safetensors failed, trying fallback...\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "    print(\"âœ… Model loaded vá»›i fallback\")\n",
    "\n",
    "print(f\"ðŸ“Š Model cÃ³ {model.num_parameters():,} parameters\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "print(f\"ðŸ”§ Model moved to {device}\")\n",
    "\n",
    "# Setup LoRA cho efficient training\n",
    "print(\"\\nðŸ”§ Setting up LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,                    # LoRA rank (nhá» hÆ¡n Ä‘á»ƒ Ä‘Æ¡n giáº£n)\n",
    "    lora_alpha=16,          # LoRA alpha\n",
    "    lora_dropout=0.1,       # LoRA dropout\n",
    "    target_modules=[\"q\", \"v\"]  # Target attention modules\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8 if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "print(\"âœ… LoRA setup hoÃ n táº¥t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc815fd7",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 5: Setup Training Arguments\n",
    "\n",
    "Cáº¥u hÃ¬nh tá»‘i Æ°u cho PyTorch 2.5.1 vÃ  dataset nhá»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73557582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£\n",
    "output_dir = \"./results/mt5-vi-en-simple\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Basic training settings\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=50,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_steps=25,\n",
    "    save_steps=25,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    \n",
    "    # Generation settings\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_length,\n",
    "    generation_num_beams=4,\n",
    "    \n",
    "    # Tá»‘i Æ°u cho PyTorch 2.5.1\n",
    "    fp16=False,  # Táº¯t fp16 cho stability\n",
    "    dataloader_num_workers=0,\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    # Save settings\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=None,  # Táº¯t wandb\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸ Training arguments:\")\n",
    "print(f\"  ðŸ“ Output: {output_dir}\")\n",
    "print(f\"  ðŸ”¢ Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  ðŸ“ Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  ðŸ“Š Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  ðŸŽ¯ Eval steps: {training_args.eval_steps}\")\n",
    "print(f\"  ðŸ”¥ FP16: {training_args.fp16}\")\n",
    "print(\"âœ… Training arguments sáºµn sÃ ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute BLEU score\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "    \n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Decode labels (replace -100 with pad token)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean text\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    \n",
    "    # Compute BLEU\n",
    "    result = bleu_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[label] for label in decoded_labels]\n",
    "    )\n",
    "    \n",
    "    # Return result\n",
    "    return {\n",
    "        \"bleu\": result[\"bleu\"],\n",
    "        \"prediction_length\": np.mean([len(pred.split()) for pred in decoded_preds])\n",
    "    }\n",
    "\n",
    "print(\"ðŸ“Š Setting up evaluation metrics...\")\n",
    "print(\"âœ… Metrics sáºµn sÃ ng - sá»­ dá»¥ng BLEU score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae92939",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 6: Training Model\n",
    "\n",
    "Báº¯t Ä‘áº§u fine-tuning vá»›i LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "print(\"ðŸš€ Táº¡o Trainer...\")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Training dataset: {len(train_dataset)} samples\")\n",
    "print(f\"ðŸ“Š Validation dataset: {len(val_dataset)} samples\")\n",
    "\n",
    "# Test model trÆ°á»›c khi training\n",
    "print(\"\\nðŸ§ª Test model trÆ°á»›c training:\")\n",
    "test_input = \"translate Vietnamese to English: Xin chÃ o\"\n",
    "inputs = tokenizer.encode(test_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs, max_length=50, num_beams=2, do_sample=False)\n",
    "    before_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"TrÆ°á»›c training: {before_translation}\")\n",
    "\n",
    "# Báº¯t Ä‘áº§u training\n",
    "print(\"\\nðŸ‹ï¸ Báº¯t Ä‘áº§u training...\")\n",
    "print(\"ðŸ’¡ Tip: Training sáº½ máº¥t vÃ i phÃºt, hÃ£y kiÃªn nháº«n!\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nâœ… Training hoÃ n táº¥t!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c901b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LÆ°u model\n",
    "print(\"ðŸ’¾ LÆ°u model...\")\n",
    "model.save_pretrained(f\"{output_dir}/final_model\")\n",
    "tokenizer.save_pretrained(f\"{output_dir}/final_model\")\n",
    "print(f\"âœ… Model Ä‘Ã£ lÆ°u táº¡i: {output_dir}/final_model\")\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ cuá»‘i cÃ¹ng\n",
    "print(\"\\nðŸ“Š ÄÃ¡nh giÃ¡ cuá»‘i cÃ¹ng...\")\n",
    "final_results = trainer.evaluate()\n",
    "print(f\"ðŸ“ˆ Final BLEU Score: {final_results.get('eval_bleu', 0):.4f}\")\n",
    "print(f\"ðŸ“‰ Final Loss: {final_results.get('eval_loss', 0):.4f}\")\n",
    "\n",
    "# Test model sau training\n",
    "print(\"\\nðŸ§ª Test model sau training:\")\n",
    "test_input = \"translate Vietnamese to English: Xin chÃ o\"\n",
    "inputs = tokenizer.encode(test_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs, max_length=50, num_beams=4, do_sample=False)\n",
    "    after_translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Sau training: {after_translation}\")\n",
    "\n",
    "# Váº½ training history náº¿u cÃ³\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Láº¥y training logs\n",
    "    logs = trainer.state.log_history\n",
    "    \n",
    "    if logs:\n",
    "        # TÃ¡ch train vÃ  eval logs\n",
    "        train_logs = [log for log in logs if 'train_loss' in log]\n",
    "        eval_logs = [log for log in logs if 'eval_loss' in log]\n",
    "        \n",
    "        if train_logs and eval_logs:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "            \n",
    "            # Training loss\n",
    "            steps = [log['step'] for log in train_logs]\n",
    "            losses = [log['train_loss'] for log in train_logs]\n",
    "            ax1.plot(steps, losses, 'b-', label='Training Loss')\n",
    "            ax1.set_title('Training Loss')\n",
    "            ax1.set_xlabel('Steps')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.legend()\n",
    "            \n",
    "            # Eval loss\n",
    "            eval_steps = [log['step'] for log in eval_logs]\n",
    "            eval_losses = [log['eval_loss'] for log in eval_logs]\n",
    "            ax2.plot(eval_steps, eval_losses, 'r-', label='Validation Loss')\n",
    "            ax2.set_title('Validation Loss')\n",
    "            ax2.set_xlabel('Steps')\n",
    "            ax2.set_ylabel('Loss')\n",
    "            ax2.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"ðŸ“Š Training charts Ä‘Æ°á»£c táº¡o!\")\n",
    "        else:\n",
    "            print(\"âš ï¸ KhÃ´ng cÃ³ Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ váº½ charts\")\n",
    "    else:\n",
    "        print(\"âš ï¸ KhÃ´ng cÃ³ training logs\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ KhÃ´ng thá»ƒ váº½ charts: {e}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training hoÃ n táº¥t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c6835",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 7: Test Translation\n",
    "\n",
    "Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»‹ch thuáº­t vá»›i cÃ¡c cÃ¢u má»›i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model\n",
    "print(\"ðŸ“Š Evaluating fine-tuned model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nðŸŽ¯ Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Táº¡o function dá»‹ch thuáº­t Ä‘Æ¡n giáº£n\n",
    "def translate_text(vietnamese_text, max_length=64, num_beams=4):\n",
    "    \"\"\"Dá»‹ch tá»« tiáº¿ng Viá»‡t sang tiáº¿ng Anh\"\"\"\n",
    "    input_text = f\"translate Vietnamese to English: {vietnamese_text}\"\n",
    "    \n",
    "    # Encode input\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=max_length, truncation=True).to(device)\n",
    "    \n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode output\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translation\n",
    "\n",
    "# Test vá»›i validation set\n",
    "print(\"ðŸ§ª Test trÃªn validation set:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(len(val_dataset)):\n",
    "    # Reconstruct original text tá»« tokenized data\n",
    "    input_ids = val_dataset[i]['input_ids']\n",
    "    label_ids = val_dataset[i]['labels']\n",
    "    \n",
    "    # Decode Ä‘á»ƒ láº¥y text gá»‘c\n",
    "    input_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    reference = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Extract Vietnamese text tá»« input\n",
    "    vietnamese_text = input_text.replace(\"translate Vietnamese to English: \", \"\")\n",
    "    \n",
    "    # Dá»‹ch\n",
    "    prediction = translate_text(vietnamese_text)\n",
    "    \n",
    "    # So sÃ¡nh (Ä‘Æ¡n giáº£n)\n",
    "    if reference.lower().strip() in prediction.lower().strip():\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "    print(f\"ðŸ‡»ðŸ‡³ Tiáº¿ng Viá»‡t: {vietnamese_text}\")\n",
    "    print(f\"ðŸŽ¯ ÄÃ¡p Ã¡n:     {reference}\")\n",
    "    print(f\"ðŸ¤– Dá»± Ä‘oÃ¡n:    {prediction}\")\n",
    "    print(f\"{'âœ… ÄÃºng' if reference.lower().strip() in prediction.lower().strip() else 'âŒ Sai'}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"\\nðŸ“Š Káº¿t quáº£:\")\n",
    "print(f\"Äá»™ chÃ­nh xÃ¡c: {accuracy:.1f}% ({correct}/{total})\")\n",
    "\n",
    "if accuracy > 70:\n",
    "    print(\"ðŸŽ‰ Tuyá»‡t vá»i! Model hoáº¡t Ä‘á»™ng tá»‘t!\")\n",
    "elif accuracy > 50:\n",
    "    print(\"ðŸ‘ KhÃ¡ tá»‘t! CÃ³ thá»ƒ cáº£i thiá»‡n thÃªm.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Cáº§n training thÃªm hoáº·c tÄƒng data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed93941",
   "metadata": {},
   "source": [
    "## BÆ°á»›c 8: Test vá»›i cÃ¢u má»›i\n",
    "\n",
    "Thá»­ nghiá»‡m vá»›i cÃ¡c cÃ¢u tiáº¿ng Viá»‡t chÆ°a cÃ³ trong training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca5c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a translation function\n",
    "def translate_vietnamese(text, max_length=128, num_beams=4):\n",
    "    \"\"\"Translate Vietnamese text to English using our fine-tuned model.\"\"\"\n",
    "    input_text = f\"translate Vietnamese to English: {text}\"\n",
    "    inputs = tokenizer.encode(\n",
    "        input_text, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=max_input_length, \n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translation\n",
    "\n",
    "# Test vá»›i cÃ¢u má»›i (khÃ´ng cÃ³ trong training data)\n",
    "test_sentences = [\n",
    "    \"ChÃ o báº¡n\",\n",
    "    \"TÃ´i Ä‘Ã³i bá»¥ng\",\n",
    "    \"BÃ¢y giá» lÃ  máº¥y giá»?\",\n",
    "    \"TÃ´i cáº§n giÃºp Ä‘á»¡\",\n",
    "    \"Ráº¥t vui Ä‘Æ°á»£c gáº·p báº¡n\",\n",
    "    \"HÃ´m nay lÃ  thá»© máº¥y?\",\n",
    "    \"TÃ´i khÃ´ng biáº¿t\",\n",
    "    \"Xin chÃ o má»i ngÆ°á»i\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ‡»ðŸ‡³ âž¡ï¸ ðŸ‡ºðŸ‡¸ TEST TRANSLATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, vn_text in enumerate(test_sentences, 1):\n",
    "    translation = translate_vietnamese(vn_text)\n",
    "    print(f\"{i}. ðŸ‡»ðŸ‡³ {vn_text}\")\n",
    "    print(f\"   ðŸ‡ºðŸ‡¸ {translation}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Interactive test - báº¡n cÃ³ thá»ƒ thay Ä‘á»•i cÃ¢u nÃ y\n",
    "print(\"\\nðŸŽ® TEST INTERACTIVE:\")\n",
    "print(\"Thay Ä‘á»•i cÃ¢u dÆ°á»›i Ä‘Ã¢y vÃ  cháº¡y láº¡i cell Ä‘á»ƒ test:\")\n",
    "\n",
    "your_sentence = \"TÃ´i yÃªu láº­p trÃ¬nh\"\n",
    "your_translation = translate_vietnamese(your_sentence)\n",
    "\n",
    "print(f\"\\nðŸ‡»ðŸ‡³ CÃ¢u cá»§a báº¡n: {your_sentence}\")\n",
    "print(f\"ðŸ‡ºðŸ‡¸ Báº£n dá»‹ch: {your_translation}\")\n",
    "\n",
    "# Tips cho ngÆ°á»i dÃ¹ng\n",
    "print(\"\\nðŸ’¡ TIPS:\")\n",
    "print(\"- Model nhá» nÃªn cÃ³ thá»ƒ chÆ°a hoÃ n háº£o\")\n",
    "print(\"- CÃ¢u cÃ ng Ä‘Æ¡n giáº£n thÃ¬ dá»‹ch cÃ ng tá»‘t\")\n",
    "print(\"- CÃ³ thá»ƒ training thÃªm Ä‘á»ƒ cáº£i thiá»‡n\")\n",
    "print(\"- Thá»­ vá»›i dataset lá»›n hÆ¡n Ä‘á»ƒ cÃ³ káº¿t quáº£ tá»‘t hÆ¡n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953b628",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Káº¿t luáº­n\n",
    "\n",
    "**ChÃºc má»«ng! Báº¡n Ä‘Ã£ hoÃ n thÃ nh viá»‡c fine-tune model dá»‹ch Viá»‡t-Anh!**\n",
    "\n",
    "### âœ… Nhá»¯ng gÃ¬ Ä‘Ã£ lÃ m:\n",
    "1. **CÃ i Ä‘áº·t**: PyTorch 2.5.1+cu121 vÃ  thÆ° viá»‡n cáº§n thiáº¿t\n",
    "2. **Dataset**: Táº¡o 20 cáº·p cÃ¢u Viá»‡t-Anh Ä‘Æ¡n giáº£n\n",
    "3. **Model**: Load mT5-small vÃ  setup LoRA\n",
    "4. **Training**: Fine-tune vá»›i settings tá»‘i Æ°u\n",
    "5. **Test**: Kiá»ƒm tra cháº¥t lÆ°á»£ng dá»‹ch thuáº­t\n",
    "\n",
    "### ðŸ“ˆ CÃ¡ch cáº£i thiá»‡n:\n",
    "\n",
    "#### 1. **Dataset lá»›n hÆ¡n**\n",
    "```python\n",
    "# Sá»­ dá»¥ng dataset tá»« Hugging Face\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"vi-en\")\n",
    "```\n",
    "\n",
    "#### 2. **Model lá»›n hÆ¡n**\n",
    "```python\n",
    "# Thá»­ mT5-base thay vÃ¬ mT5-small\n",
    "MODEL_NAME = \"google/mt5-base\"\n",
    "```\n",
    "\n",
    "#### 3. **Training lÃ¢u hÆ¡n**\n",
    "```python\n",
    "# TÄƒng epochs vÃ  giáº£m learning rate\n",
    "num_train_epochs=10\n",
    "learning_rate=3e-4\n",
    "```\n",
    "\n",
    "#### 4. **Hyperparameter tuning**\n",
    "```python\n",
    "# Thá»­ cÃ¡c settings khÃ¡c\n",
    "r=16, lora_alpha=32  # LoRA settings\n",
    "per_device_train_batch_size=4  # Batch size\n",
    "```\n",
    "\n",
    "### ðŸš€ BÆ°á»›c tiáº¿p theo:\n",
    "\n",
    "1. **Sá»­ dá»¥ng script cÃ³ sáºµn**:\n",
    "   ```bash\n",
    "   # Preprocess data\n",
    "   python -m src.data_processing.preprocess\n",
    "   \n",
    "   # Training\n",
    "   python -m src.training.fine_tune\n",
    "   \n",
    "   # Inference\n",
    "   python -m src.inference.translate --model_path \"path/to/model\" --text \"xin chÃ o\"\n",
    "   ```\n",
    "\n",
    "2. **Cháº¡y web interface**:\n",
    "   ```bash\n",
    "   python main.py\n",
    "   # Má»Ÿ http://localhost:8000\n",
    "   ```\n",
    "\n",
    "3. **Deploy API**:\n",
    "   ```bash\n",
    "   uvicorn main:app --host 0.0.0.0 --port 8000\n",
    "   ```\n",
    "\n",
    "### ðŸŽ¯ Káº¿t quáº£ mong Ä‘á»£i:\n",
    "- **Small dataset**: BLEU ~0.3-0.5\n",
    "- **Large dataset**: BLEU ~0.6-0.8\n",
    "- **Production ready**: BLEU >0.8\n",
    "\n",
    "### ðŸ“š TÃ i liá»‡u tham kháº£o:\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [mT5 Paper](https://arxiv.org/abs/2010.11934)\n",
    "\n",
    "### ðŸ”§ Troubleshooting:\n",
    "- **Out of memory**: Giáº£m batch_size xuá»‘ng 1\n",
    "- **Slow training**: Sá»­ dá»¥ng GPU hoáº·c giáº£m model size\n",
    "- **Poor quality**: TÄƒng data hoáº·c training epochs\n",
    "\n",
    "**Happy coding! ðŸŽ‰**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
