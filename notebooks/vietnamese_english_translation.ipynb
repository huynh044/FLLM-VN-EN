{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11cab4da",
   "metadata": {},
   "source": [
    "# Fine-tuning LLM for Vietnamese to English Translation\n",
    "\n",
    "This notebook demonstrates how to fine-tune a large language model (LLM) for translating Vietnamese text to English. We'll use transformer models like mT5 or MarianMT with the Hugging Face ecosystem.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "- **Objective**: Fine-tune a pre-trained model for Vietnamese-English translation\n",
    "- **Models**: mT5, MarianMT, or similar sequence-to-sequence models\n",
    "- **Dataset**: Vietnamese-English parallel corpus\n",
    "- **Evaluation**: BLEU score and qualitative assessment\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Set up environment and install dependencies\n",
    "2. Load and preprocess Vietnamese-English translation dataset\n",
    "3. Tokenize and prepare data for model input\n",
    "4. Load pre-trained LLM model and tokenizer\n",
    "5. Configure training arguments for fine-tuning\n",
    "6. Fine-tune the model on translation data\n",
    "7. Evaluate model performance on validation set\n",
    "8. Translate sample Vietnamese sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8af4ff",
   "metadata": {},
   "source": [
    "## 1. Set Up Environment and Install Dependencies\n",
    "\n",
    "First, let's install all the required libraries for our translation project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers datasets accelerate peft bitsandbytes\n",
    "!pip install rouge-score evaluate\n",
    "!pip install pandas numpy matplotlib seaborn plotly\n",
    "!pip install pyvi underthesea  # Vietnamese NLP libraries\n",
    "!pip install tqdm wandb tensorboard  # Utilities\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d51d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers and related libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import evaluate\n",
    "\n",
    "# PEFT for parameter-efficient fine-tuning\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Vietnamese text processing\n",
    "try:\n",
    "    from pyvi import ViTokenizer\n",
    "    from underthesea import word_tokenize as vn_tokenize\n",
    "    print(\"‚úÖ Vietnamese NLP libraries loaded\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Vietnamese NLP libraries not available\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üîß Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üìä GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3d581",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Vietnamese-English Translation Dataset\n",
    "\n",
    "We'll create a sample dataset and also show how to load real parallel corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1838dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample Vietnamese-English dataset\n",
    "sample_data = [\n",
    "    {\"vietnamese\": \"Xin ch√†o! T√¥i t√™n l√† Nam.\", \"english\": \"Hello! My name is Nam.\"},\n",
    "    {\"vietnamese\": \"H√¥m nay th·ªùi ti·∫øt r·∫•t ƒë·∫πp.\", \"english\": \"The weather is very nice today.\"},\n",
    "    {\"vietnamese\": \"T√¥i ƒëang h·ªçc ti·∫øng Anh.\", \"english\": \"I am learning English.\"},\n",
    "    {\"vietnamese\": \"B·∫°n c√≥ kh·ªèe kh√¥ng?\", \"english\": \"How are you?\"},\n",
    "    {\"vietnamese\": \"C·∫£m ∆°n b·∫°n r·∫•t nhi·ªÅu.\", \"english\": \"Thank you very much.\"},\n",
    "    {\"vietnamese\": \"Xin l·ªói, t√¥i kh√¥ng hi·ªÉu.\", \"english\": \"Sorry, I don't understand.\"},\n",
    "    {\"vietnamese\": \"B·∫°n c√≥ th·ªÉ gi√∫p t√¥i ƒë∆∞·ª£c kh√¥ng?\", \"english\": \"Can you help me?\"},\n",
    "    {\"vietnamese\": \"T√¥i mu·ªën ƒëi du l·ªãch Vi·ªát Nam.\", \"english\": \"I want to travel to Vietnam.\"},\n",
    "    {\"vietnamese\": \"M√≥n ph·ªü n√†y r·∫•t ngon.\", \"english\": \"This pho is very delicious.\"},\n",
    "    {\"vietnamese\": \"Ch√∫c b·∫°n m·ªôt ng√†y t·ªët l√†nh.\", \"english\": \"Have a nice day.\"},\n",
    "    {\"vietnamese\": \"T√¥i y√™u vƒÉn h√≥a Vi·ªát Nam.\", \"english\": \"I love Vietnamese culture.\"},\n",
    "    {\"vietnamese\": \"H√† N·ªôi l√† th·ªß ƒë√¥ c·ªßa Vi·ªát Nam.\", \"english\": \"Hanoi is the capital of Vietnam.\"},\n",
    "    {\"vietnamese\": \"T√¥i th√≠ch ƒÉn b√°nh m√¨.\", \"english\": \"I like to eat banh mi.\"},\n",
    "    {\"vietnamese\": \"Ng√†y mai t√¥i s·∫Ω ƒëi l√†m.\", \"english\": \"Tomorrow I will go to work.\"},\n",
    "    {\"vietnamese\": \"Gia ƒë√¨nh t√¥i c√≥ b·ªën ng∆∞·ªùi.\", \"english\": \"My family has four people.\"},\n",
    "    {\"vietnamese\": \"T√¥i h·ªçc t·∫°i tr∆∞·ªùng ƒë·∫°i h·ªçc.\", \"english\": \"I study at the university.\"},\n",
    "    {\"vietnamese\": \"Xe bu√Ωt s·∫Ω ƒë·∫øn l√∫c 8 gi·ªù.\", \"english\": \"The bus will arrive at 8 o'clock.\"},\n",
    "    {\"vietnamese\": \"T√¥i c·∫ßn mua m·ªôt cu·ªën s√°ch.\", \"english\": \"I need to buy a book.\"},\n",
    "    {\"vietnamese\": \"B√°c sƒ© n√≥i t√¥i kh·ªèe m·∫°nh.\", \"english\": \"The doctor says I'm healthy.\"},\n",
    "    {\"vietnamese\": \"Ch√∫ng t√¥i s·∫Ω g·∫∑p nhau t·ªëi nay.\", \"english\": \"We will meet tonight.\"}\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(sample_data)\n",
    "print(f\"üìä Created sample dataset with {len(df)} translation pairs\")\n",
    "\n",
    "# Display first few examples\n",
    "print(\"\\nüîç Sample translation pairs:\")\n",
    "for i in range(5):\n",
    "    print(f\"üáªüá≥ Vietnamese: {df.iloc[i]['vietnamese']}\")\n",
    "    print(f\"üá∫üá∏ English: {df.iloc[i]['english']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "import re\n",
    "\n",
    "def clean_vietnamese_text(text):\n",
    "    \"\"\"Clean Vietnamese text.\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters (keep Vietnamese diacritics)\n",
    "    text = re.sub(r'[^\\w\\s\\u00C0-\\u1EF9]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_english_text(text):\n",
    "    \"\"\"Clean English text.\"\"\"\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters (keep basic punctuation)\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing\n",
    "df['vietnamese_clean'] = df['vietnamese'].apply(clean_vietnamese_text)\n",
    "df['english_clean'] = df['english'].apply(clean_english_text)\n",
    "\n",
    "# Analyze text lengths\n",
    "df['vn_length'] = df['vietnamese_clean'].str.len()\n",
    "df['en_length'] = df['english_clean'].str.len()\n",
    "df['vn_words'] = df['vietnamese_clean'].str.split().str.len()\n",
    "df['en_words'] = df['english_clean'].str.split().str.len()\n",
    "\n",
    "# Display statistics\n",
    "print(\"üìà Dataset Statistics:\")\n",
    "print(f\"Average Vietnamese length: {df['vn_length'].mean():.1f} characters\")\n",
    "print(f\"Average English length: {df['en_length'].mean():.1f} characters\")\n",
    "print(f\"Average Vietnamese words: {df['vn_words'].mean():.1f} words\")\n",
    "print(f\"Average English words: {df['en_words'].mean():.1f} words\")\n",
    "\n",
    "# Visualize length distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].hist(df['vn_words'], bins=10, alpha=0.7, label='Vietnamese', color='blue')\n",
    "axes[0].hist(df['en_words'], bins=10, alpha=0.7, label='English', color='red')\n",
    "axes[0].set_xlabel('Number of Words')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Word Count Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(df['vn_words'], df['en_words'], alpha=0.7)\n",
    "axes[1].set_xlabel('Vietnamese Words')\n",
    "axes[1].set_ylabel('English Words')\n",
    "axes[1].set_title('Vietnamese vs English Word Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c4ae0b",
   "metadata": {},
   "source": [
    "## 3. Tokenize and Prepare Data for Model Input\n",
    "\n",
    "We'll use a multilingual T5 (mT5) tokenizer to process our Vietnamese-English data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model and load tokenizer\n",
    "MODEL_NAME = \"google/mt5-small\"  # Start with small model for testing\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"ü§ñ Loaded tokenizer: {MODEL_NAME}\")\n",
    "print(f\"üìö Vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "# Test tokenization on a sample\n",
    "sample_vn = df.iloc[0]['vietnamese_clean']\n",
    "sample_en = df.iloc[0]['english_clean']\n",
    "\n",
    "print(f\"\\nüß™ Tokenization Test:\")\n",
    "print(f\"Vietnamese: {sample_vn}\")\n",
    "vn_tokens = tokenizer.tokenize(sample_vn)\n",
    "print(f\"VN Tokens: {vn_tokens}\")\n",
    "print(f\"VN Token IDs: {tokenizer.encode(sample_vn)}\")\n",
    "\n",
    "print(f\"\\nEnglish: {sample_en}\")\n",
    "en_tokens = tokenizer.tokenize(sample_en)\n",
    "print(f\"EN Tokens: {en_tokens}\")\n",
    "print(f\"EN Token IDs: {tokenizer.encode(sample_en)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data in T5 format: \"translate Vietnamese to English: [VN_TEXT]\" -> \"[EN_TEXT]\"\n",
    "def create_t5_format(vietnamese_text, english_text):\n",
    "    \"\"\"Create T5 input-output format.\"\"\"\n",
    "    input_text = f\"translate Vietnamese to English: {vietnamese_text}\"\n",
    "    target_text = english_text\n",
    "    return input_text, target_text\n",
    "\n",
    "# Create T5 formatted data\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    input_text, target_text = create_t5_format(\n",
    "        row['vietnamese_clean'], \n",
    "        row['english_clean']\n",
    "    )\n",
    "    inputs.append(input_text)\n",
    "    targets.append(target_text)\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "dataset_dict = {\n",
    "    'input_text': inputs,\n",
    "    'target_text': targets,\n",
    "    'vietnamese': df['vietnamese_clean'].tolist(),\n",
    "    'english': df['english_clean'].tolist()\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Split into train/validation sets (80/20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "train_dataset = dataset.select(range(train_size))\n",
    "val_dataset = dataset.select(range(train_size, len(dataset)))\n",
    "\n",
    "dataset_splits = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "print(f\"üìä Dataset splits:\")\n",
    "print(f\"  Training: {len(train_dataset)} examples\")\n",
    "print(f\"  Validation: {len(val_dataset)} examples\")\n",
    "\n",
    "# Show example of formatted data\n",
    "print(f\"\\nüîç Example formatted data:\")\n",
    "example = train_dataset[0]\n",
    "print(f\"Input: {example['input_text']}\")\n",
    "print(f\"Target: {example['target_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization preprocessing function\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize inputs and targets.\"\"\"\n",
    "    inputs = examples['input_text']\n",
    "    targets = examples['target_text']\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=False  # We'll pad dynamically during training\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization to datasets\n",
    "tokenized_datasets = dataset_splits.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_splits[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenization completed\")\n",
    "print(f\"üìä Tokenized dataset structure:\")\n",
    "print(tokenized_datasets)\n",
    "\n",
    "# Show tokenized example\n",
    "example = tokenized_datasets['train'][0]\n",
    "print(f\"\\nüîç Tokenized example:\")\n",
    "print(f\"Input IDs shape: {len(example['input_ids'])}\")\n",
    "print(f\"Label IDs shape: {len(example['labels'])}\")\n",
    "print(f\"First few input tokens: {tokenizer.convert_ids_to_tokens(example['input_ids'][:10])}\")\n",
    "print(f\"First few label tokens: {tokenizer.convert_ids_to_tokens(example['labels'][:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3dbb41",
   "metadata": {},
   "source": [
    "## 4. Load Pre-trained LLM Model and Tokenizer\n",
    "\n",
    "We'll load the mT5 model and set it up for translation fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "print(f\"ü§ñ Loaded model: {MODEL_NAME}\")\n",
    "print(f\"üìä Model parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up LoRA (Parameter-Efficient Fine-Tuning)\n",
    "USE_PEFT = True  # Set to False for full fine-tuning\n",
    "\n",
    "if USE_PEFT:\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=16,                    # LoRA rank\n",
    "        lora_alpha=32,          # LoRA alpha\n",
    "        lora_dropout=0.1,       # LoRA dropout\n",
    "        target_modules=[\"q\", \"v\"]  # Target attention modules\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    print(\"‚úÖ LoRA configuration applied\")\n",
    "else:\n",
    "    print(\"üìö Using full fine-tuning\")\n",
    "\n",
    "# Setup data collator for dynamic padding\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8 if torch.cuda.is_available() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc815fd7",
   "metadata": {},
   "source": [
    "## 5. Configure Training Arguments for Fine-Tuning\n",
    "\n",
    "Set up training parameters optimized for our translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73557582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "output_dir = \"./results/mt5-vietnamese-english\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Training configuration\n",
    "    num_train_epochs=10,  # More epochs for small dataset\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    # Learning rate and optimization\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    logging_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    \n",
    "    # Model selection\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_bleu\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    # Generation parameters\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,\n",
    "    generation_num_beams=4,\n",
    "    \n",
    "    # Performance optimizations\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=0,  # Reduce for small dataset\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    # Reporting\n",
    "    report_to=None,  # Disable wandb for this demo\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è Training arguments configured:\")\n",
    "print(f\"  üìÅ Output directory: {output_dir}\")\n",
    "print(f\"  üî¢ Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  üìè Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  üìä Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  üéØ Evaluation steps: {training_args.eval_steps}\")\n",
    "print(f\"  üíæ Save steps: {training_args.save_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute BLEU and ROUGE metrics for evaluation.\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up text\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    bleu_result = bleu_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[label] for label in decoded_labels]\n",
    "    )\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    rouge_result = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels\n",
    "    )\n",
    "    \n",
    "    # Combine results\n",
    "    result = {\n",
    "        \"bleu\": bleu_result[\"bleu\"],\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"]\n",
    "    }\n",
    "    \n",
    "    # Add generation length info\n",
    "    prediction_lens = [len(pred.split()) for pred in decoded_preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"üìä Evaluation metrics configured:\")\n",
    "print(\"  - BLEU score for translation quality\")\n",
    "print(\"  - ROUGE scores for text similarity\")\n",
    "print(\"  - Generation length statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae92939",
   "metadata": {},
   "source": [
    "## 6. Fine-Tune the Model on Translation Data\n",
    "\n",
    "Now we'll train our model on the Vietnamese-English dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Trainer initialized!\")\n",
    "print(f\"üìä Training dataset size: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"üìä Validation dataset size: {len(tokenized_datasets['validation'])}\")\n",
    "\n",
    "# Test the model before training\n",
    "print(\"\\nüß™ Testing model before training:\")\n",
    "test_input = \"translate Vietnamese to English: Xin ch√†o b·∫°n!\"\n",
    "inputs = tokenizer.encode(test_input, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs, max_length=50, num_beams=4)\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Before training: {translation}\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nüèãÔ∏è Starting training...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c901b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(f\"{output_dir}/final_model\")\n",
    "tokenizer.save_pretrained(f\"{output_dir}/final_model\")\n",
    "\n",
    "print(\"üíæ Model saved successfully!\")\n",
    "\n",
    "# Plot training history\n",
    "training_history = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "if len(training_history) > 0:\n",
    "    # Separate training and evaluation logs\n",
    "    train_logs = training_history[training_history['train_loss'].notna()]\n",
    "    eval_logs = training_history[training_history['eval_loss'].notna()]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training loss\n",
    "    if len(train_logs) > 0:\n",
    "        axes[0, 0].plot(train_logs['step'], train_logs['train_loss'], 'b-', label='Training Loss')\n",
    "        axes[0, 0].set_title('Training Loss')\n",
    "        axes[0, 0].set_xlabel('Steps')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "    \n",
    "    # Evaluation loss\n",
    "    if len(eval_logs) > 0:\n",
    "        axes[0, 1].plot(eval_logs['step'], eval_logs['eval_loss'], 'r-', label='Validation Loss')\n",
    "        axes[0, 1].set_title('Validation Loss')\n",
    "        axes[0, 1].set_xlabel('Steps')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].legend()\n",
    "    \n",
    "    # BLEU score\n",
    "    if len(eval_logs) > 0 and 'eval_bleu' in eval_logs.columns:\n",
    "        axes[1, 0].plot(eval_logs['step'], eval_logs['eval_bleu'], 'g-', label='BLEU Score')\n",
    "        axes[1, 0].set_title('BLEU Score')\n",
    "        axes[1, 0].set_xlabel('Steps')\n",
    "        axes[1, 0].set_ylabel('BLEU')\n",
    "        axes[1, 0].legend()\n",
    "    \n",
    "    # Learning rate\n",
    "    if len(train_logs) > 0 and 'learning_rate' in train_logs.columns:\n",
    "        axes[1, 1].plot(train_logs['step'], train_logs['learning_rate'], 'm-', label='Learning Rate')\n",
    "        axes[1, 1].set_title('Learning Rate')\n",
    "        axes[1, 1].set_xlabel('Steps')\n",
    "        axes[1, 1].set_ylabel('Learning Rate')\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training history available for plotting\")\n",
    "\n",
    "print(\"‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183c6835",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance on Validation Set\n",
    "\n",
    "Let's evaluate our fine-tuned model and compare it with the pre-trained version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fa0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model\n",
    "print(\"üìä Evaluating fine-tuned model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nüéØ Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Detailed evaluation on validation set\n",
    "validation_data = dataset_splits['validation']\n",
    "print(f\"\\nüîç Detailed evaluation on {len(validation_data)} examples:\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "vietnamese_texts = []\n",
    "\n",
    "for i, example in enumerate(validation_data):\n",
    "    vietnamese_text = example['vietnamese']\n",
    "    english_text = example['english']\n",
    "    \n",
    "    # Create input for the model\n",
    "    input_text = f\"translate Vietnamese to English: {vietnamese_text}\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=max_input_length, truncation=True).to(device)\n",
    "    \n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_target_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    # Decode prediction\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "    references.append(english_text)\n",
    "    vietnamese_texts.append(vietnamese_text)\n",
    "\n",
    "# Calculate metrics\n",
    "final_bleu = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "final_rouge = rouge_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(f\"\\nüìà Final Metrics:\")\n",
    "print(f\"  BLEU Score: {final_bleu['bleu']:.4f}\")\n",
    "print(f\"  ROUGE-1: {final_rouge['rouge1']:.4f}\")\n",
    "print(f\"  ROUGE-2: {final_rouge['rouge2']:.4f}\")\n",
    "print(f\"  ROUGE-L: {final_rouge['rougeL']:.4f}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nüåü Translation Examples:\")\n",
    "print(\"=\" * 80)\n",
    "for i in range(min(5, len(predictions))):\n",
    "    print(f\"Vietnamese: {vietnamese_texts[i]}\")\n",
    "    print(f\"Reference:  {references[i]}\")\n",
    "    print(f\"Predicted:  {predictions[i]}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed93941",
   "metadata": {},
   "source": [
    "## 8. Translate Sample Vietnamese Sentences\n",
    "\n",
    "Let's test our fine-tuned model with some new Vietnamese sentences and see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca5c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a translation function\n",
    "def translate_vietnamese(text, max_length=128, num_beams=4):\n",
    "    \"\"\"Translate Vietnamese text to English using our fine-tuned model.\"\"\"\n",
    "    input_text = f\"translate Vietnamese to English: {text}\"\n",
    "    inputs = tokenizer.encode(\n",
    "        input_text, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=max_input_length, \n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translation\n",
    "\n",
    "# Test sentences (not in training data)\n",
    "test_sentences = [\n",
    "    \"T√¥i r·∫•t th√≠ch m√≥n ph·ªü Vi·ªát Nam.\",\n",
    "    \"H√¥m nay tr·ªùi m∆∞a to l·∫Øm.\",\n",
    "    \"B·∫°n c√≥ th·ªÉ cho t√¥i bi·∫øt ƒë∆∞·ªùng ƒëi kh√¥ng?\",\n",
    "    \"Ch√∫ng t√¥i s·∫Ω g·∫∑p nhau v√†o cu·ªëi tu·∫ßn.\",\n",
    "    \"Ti·∫øng Vi·ªát l√† ng√¥n ng·ªØ r·∫•t hay.\",\n",
    "    \"T√¥i mu·ªën h·ªçc l·∫≠p tr√¨nh m√°y t√≠nh.\",\n",
    "    \"Cu·ªôc s·ªëng ·ªü th√†nh ph·ªë r·∫•t n√°o nhi·ªát.\",\n",
    "    \"Gia ƒë√¨nh l√† ƒëi·ªÅu quan tr·ªçng nh·∫•t.\"\n",
    "]\n",
    "\n",
    "print(\"üáªüá≥ ‚û°Ô∏è üá∫üá∏ Vietnamese to English Translation Results\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, vietnamese_text in enumerate(test_sentences, 1):\n",
    "    translation = translate_vietnamese(vietnamese_text)\n",
    "    print(f\"{i}. Vietnamese: {vietnamese_text}\")\n",
    "    print(f\"   English:   {translation}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# Interactive translation (you can modify this cell to test your own sentences)\n",
    "print(\"\\nüöÄ Try your own translations!\")\n",
    "print(\"Modify the sentence below and run the cell to see the translation:\")\n",
    "\n",
    "custom_sentence = \"T√¥i y√™u Vi·ªát Nam v√† vƒÉn h√≥a truy·ªÅn th·ªëng.\"\n",
    "custom_translation = translate_vietnamese(custom_sentence)\n",
    "\n",
    "print(f\"\\nüáªüá≥ Vietnamese: {custom_sentence}\")\n",
    "print(f\"üá∫üá∏ English: {custom_translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953b628",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "üéâ **Congratulations!** You have successfully fine-tuned a large language model for Vietnamese to English translation.\n",
    "\n",
    "### What we accomplished:\n",
    "- ‚úÖ Set up the environment with all necessary libraries\n",
    "- ‚úÖ Created and preprocessed a Vietnamese-English dataset\n",
    "- ‚úÖ Tokenized data using mT5 tokenizer\n",
    "- ‚úÖ Loaded and configured a pre-trained mT5 model\n",
    "- ‚úÖ Applied LoRA for parameter-efficient fine-tuning\n",
    "- ‚úÖ Trained the model with proper evaluation metrics\n",
    "- ‚úÖ Evaluated performance using BLEU and ROUGE scores\n",
    "- ‚úÖ Tested translations on new Vietnamese sentences\n",
    "\n",
    "### Next Steps to Improve:\n",
    "\n",
    "1. **Larger Dataset**: \n",
    "   - Use larger parallel corpora (OPUS, OpenSubtitles, etc.)\n",
    "   - Implement the data download script in `scripts/download_data.py`\n",
    "\n",
    "2. **Better Models**: \n",
    "   - Try larger models (mT5-base, mT5-large)\n",
    "   - Experiment with other architectures (MarianMT, NLLB)\n",
    "\n",
    "3. **Advanced Techniques**:\n",
    "   - Data augmentation (back-translation)\n",
    "   - Multi-task learning\n",
    "   - Domain adaptation\n",
    "\n",
    "4. **Production Deployment**:\n",
    "   - Use the inference script in `src/inference/translate.py`\n",
    "   - Deploy as a web service or API\n",
    "   - Optimize for speed and memory usage\n",
    "\n",
    "### Resources:\n",
    "- üìñ [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers)\n",
    "- üîß [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- üìä [Evaluate Library](https://huggingface.co/docs/evaluate)\n",
    "- üåê [OPUS Parallel Corpora](http://opus.nlpl.eu/)\n",
    "\n",
    "### Model Performance Tips:\n",
    "- Monitor validation loss to avoid overfitting\n",
    "- Experiment with different learning rates\n",
    "- Try different beam search settings for generation\n",
    "- Consider ensembling multiple models for better results\n",
    "\n",
    "Happy translating! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
